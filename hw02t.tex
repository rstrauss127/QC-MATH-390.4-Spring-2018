\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
%\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 390.4 / 650.2 Spring 2018 Homework \#2t}

\author{Professor Adam Kapelner} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due 11:59PM Tuesday, March 6, 2018 under the door of KY604 \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}


\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, read about all the concepts introduced in class online. This is your responsibility to supplement in-class with your own readings. There are no pop-book readings this homework so you have more time to study for the exam.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 10 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex \emph{and} preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}
\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}

\problem{These are questions about the SVM.}

\begin{enumerate}

\easysubproblem{State the hypothesis set $\mathcal{H}$ inputted into the support vector machine algorithm. Is it different than the $\mathcal{H}$ used for $\mathcal{A}$ = perceptron learning algorithm?}\spc{1}

$\mathcal{H}$ = {${\mathbbm{1}_{\vec{w}\odot\vec{x}+b>0}}, \vec{w}\in\mathbb{R}^p, b\in\mathbb{R }$}

\extracreditsubproblem{Why is the SVM better than the perceptron? A non-technical discussion that makes sense is fine. Write it on a separate page}
Perceptron tries to minimize the error while the SVM tries to maximize the margin

\hardsubproblem{Let $\mathcal{Y} = \braces{-1,1}$. Rederive the cost function whose minimization yields the SVM line in the linearly separable case. }\spc{20}

using Hesse normal form
$\mathcal{H}= {{\mathbb{1}_{\vec{w}\dot\vec{x}-b=0}}:\vec{w}\in\mathbb{R}, b\in\mathbb{R}}$. Since data is linearly seperable, we can draw two "support vector lines" to create the boundary lines for the "support vector machine" line/plane. 
The equation of the SVM line/plane is \begin{equation}
\vec{w}\cdot\vec{x}-b=0
\end{equation}
The equation of the uuper line is \begin{equation}
\vec{w}\cdot\vec{x}-(b+\delta)=0 
\end{equation}
and the equation of the lower line is \begin{equation}
\vec{w}\cdot\vec{x}-(b-\delta)=0 
\end{equation}
where $\delta$ is the distance between the each boundary line and the middle line.

Now we constrain all $y=1$ to be greater than or equal to the upper line and all $y=-1$ to be less than or equal to the lower line.\\

\begin{equation}
y_i*(\vec{w}\cdot\vec{x}-b)\geq y_i*\delta
\end{equation}
$\forall$i s.t $y_i=1$, $y_i*(\vec{w}\cdot\vec{x}-b)\geq\delta$\\
and $\forall$i s.t $y_i=-1$,
$-1*(\vec{w}\cdot\vec{x}-b)\leq-1*\delta$. Multiply both sides by $-1$, flip the inequality sign and we get $\vec{w}\cdot\vec{x}-b\geq\delta$

\easysubproblem{Given your answer to (c) rederive the cost function using the \qu{soft margin} i.e. the hinge loss plus the term with the hyperparameter $\lambda$. This is marked easy since there is just one change from the expression given in class.}\spc{4}

\end{enumerate}



\problem{These are questions are about the $k$ nearest neighbors (KNN) algorithm.}

\begin{enumerate}

\easysubproblem{Describe how the algorithm works. Is $k$ a \qu{hyperparameter}?}\spc{5}

Given the usual suspets, we want to predict $\hat{y}^* = g(\vec{x}^*)$. KNN finds a predefined amount of $x_i\in\mathbb{D}$ and returns $\hat{y} = MODEL[y(1), ... y(k)]$ where each $y_i$ represents the "closest" or most "similiar" $x_i$s. Since $k$ tells $\mathbb{A}$ what to return, it can be considered a tuning knob or a hyper parameter.

\hardsubproblem{Assuming $\mathcal{A} = $ KNN, describe the input $\mathcal{H}$ as best as you can.}\spc{8}
The set of all classes we can classify the data with.
\hardsubproblem{When predicting on $\mathbb{D}$ with $k=1$, why should there be zero error? Is this a good estimate of future error when new data comes in? (Error in the future is called \emph{generalization error} and we will be discussing this later in the semester).}\spc{5}

Using $k = 1$ for predictions on $\mathbb{D}$, there should always be zero error. This is because $\mathbb{A}$ is taking in a variable from $\mathbb{D}$ and using that same set $\mathbb{D}$ to pull the nearest neighbor from and the nearest neighbor to anything is always itself. Having zero error is a bad thing because it means that your model will fail when exposed to future data since we "overfitted" the model, made it to specific.

\end{enumerate}

\problem{These are questions about the linear model with $p=1$.}

\begin{enumerate}

\easysubproblem{What does $\mathbb{D}$ look like in the linear model with $p=1$? What is $\mathcal{X}$? What is $\mathcal{Y}$?}\spc{3}
$\mathcal{X} = [x_1, x_2,.., x_n]$ and $\mathcal{Y} = [y_1, y_2,...y_n]$ where $\forall{y_i}: y\in\mathbb{R}$. Then $\mathbb{D} =[<1, 0>, <x_1, y_1>, .. <x_n, y_n>]$

\easysubproblem{Consider the line fit using the ordinary least squares (OLS) algorithm. Prove that the point $<\xbar, \ybar>$ is on this line. Use the formulas we derived in class.}\spc{3}
We know that the line fit using OLS is \begin{equation} g(x) = y = b_0 + b_1x \end{equation}
and \begin{equation}
\xbar = \frac{1}{n}\sum\limits_{i=1}^n x_i
\end{equation}
and \begin{equation}
\ybar = \frac{1}{n}\sum\limits_{i=1}^n y_i
\end{equation}
$g(x)$ comes from minimimizing the SSE equation \begin{equation}
\sum\limits_{i=1}^n (y_i - (b_0+b_1x_i))^2
\end{equation}
If we foil this out \begin{equation}
\sum\limits_{i=1}^n y_i^2 + b_0^2 +b_1^2\sum\limits_{i=1}^nx_i^2-2b_0\sum\limits_{i=1}^ny_i-2b_1\sum\limits_{i=1}^ny_ix_i+2b_0b_1\sum\limits_{i=1}^nx_i 
\end{equation}
Now we can replace all the indivdivial sums of $x_i$ and $y_i$ with equations $n*\xbar$ and $n*\ybar$ respectively, also distribute sum of constants. \begin{equation}
\sum\limits_{i=1}^ny_i^2+ nb_0^2+ b_1^2\sum\limits_{i=1}^nx_i^2-2nb_0\ybar-2b_1\sum\limits_{i=1}^ny_ix_i+2nb_0b_1\xbar
\end{equation}
To get the OLS line, we need to minimize this equation. Take the partial derivate of the equation above with respect to $b_0$ and set it equal to zero
\begin{equation}
-2n\ybar+2nb_0+2nb_1\xbar = 0
\end{equation}
Solving for $b_0$ gives us \begin{equation}
b_0 = \ybar-b_1\xbar
\end{equation}
Since any line in 2D space is a linear combination of it's constants,the line will pass through all points that are multiples of these linear combiniations. Since one of our constants, $b_0$ is a function of $\ybar$ and $\xbar$, our OLS line $g(x)=b_0+b_1x$ has to pass through the point $<\xbar, \ybar>$
\intermediatesubproblem{Consider the line fit using OLS. Prove that the average prediction $\hat{y}_i := g(x_i)$ for $x_i \in \mathbb{D}$ is $\ybar$.}\spc{4}

During the OLS process, we are generating the line that has the sum of its' indivudal residuals(distance from the ith point on the line to the ith data point) equal to 0, $\frac{1}{n}\sum\limits_{i=1}^n(y_i-\hat{y_i})^2 = 0$. If we are prediciting for some $x_i\in\mathbb{D}$, then we should use the sample average $\bar{y_i}$. Since on average $\sum\limits_{i=1}^ny_i=\sum\limits_{i=1}^n\hat{y_i}$, we can say that $\ybar=\bar{y_i}$, our average prediction for $x_i$ is $\ybar$
\intermediatesubproblem{Consider the line fit using OLS. Prove that the average residual $e_i$ computed from all predictions for $x_i \in \mathbb{D}$ and its true response value $y_i$ is 0.}\spc{3}
\setcounter{equation}{0}
OLS line is $y=b_0+b_1x$. where \begin{equation}
b_0=\ybar-b_1\xbar 
\end{equation}                                       
and \begin{equation}
\ybar=\frac{1}{n}\sum\limits_{i=1}^ny_i 
\end{equation}
and \begin{equation}
\xbar=\frac{1}{n}\sum\limits_{i=1}^nx_i
\end{equation}
We want to prove that \begin{equation}
\sum\limits_{i=1}^ne_i=\sum\limits_{i=1}^n(y_i-b_0-b_1x_i)=0
\end{equation}

Plug $b_0$ into equation 4 \begin{equation*}
\sum\limits_{i=1}^n(y_i-\ybar+b_1\xbar-b_ix_i)
\end{equation*}
Distribute sum and Pull out the constants
\begin{equation*}
\sum\limits_{i=1}^ny_i-\ybar\sum\limits_{i=1}^n1+b_1\xbar\sum\limits_{i=1}^n1-b_1\sum\limits_{i=1}^nx_i
\end{equation*}
Rewrite \begin{equation*}
\sum\limits_{i=1}^ny_i-n\ybar+nb_1\xbar-b_1\sum\limits_{i=1}^nx_i
\end{equation*}
Plug equation 2 and 3 in
\begin{equation*}
\sum\limits_{i=1}^ny_i-(\frac{1}{n}\sum\limits_{i=1}^ny_i)n+(\frac{1}{n}\sum\limits_{i=1}^nx_i)nb_1-b_1\sum\limits_{i=1}^nx_i
\end{equation*}
cancel the n's
\begin{equation*}
\sum\limits_{i=1}^ne_i=\sum\limits_{i=1}^ny_i-\sum\limits_{i=1}^ny_i+b_1\sum\limits_{i=1}^nx_i-b_1\sum\limits_{i=1}^nx_i = 0
\end{equation*}
Hence \begin{equation}
\sum\limits_{i=1}^ne_i=0
\end{equation}

\intermediatesubproblem{Why is the RMSE usually a better indicator of predictive performance than $R^2$? Discuss in English.}\spc{4}

$R^2$ is relative because it is unit less, so there is no way to compare models using this metric. Additionally, the $R^2$ value gets inflated the more features we add to our model. $R^2$ \\
RMSE measures how close your predicted values are the real data we are trying to model so RMSE can be viewed as a measure of how accurate your model is. RMSE is sesitive to large errors-penalizes based on the size of the error. RMSE is easier to communicate to someone who klnows nothing about statistics because its' reported value is in the same units as the depenedent variables being modelled.


\intermediatesubproblem{$R^2$ is commonly interpreted as \qu{proportion of the variance explained by the model} and proportions are constrained to the interval $\zeroonecl$. While it is true that $R^2 \leq 1$ for all models, it is not true that $R^2 \geq 0$ for all models. Construct an explicit example $\mathbb{D}$ and create a linear model $g(x) = w_0 + w_1 x$ whose $R^2 < 0$. Hint: do not use the OLS line. Hint: draw a picture!}\spc{10}
$\mathbb{D}= {<1,5>, <2,5>, <3,5>}$ and $\vec{w}=[5, -1]$
\extracreditsubproblem{Prove that the OLS line always has $R^2 \in \zeroonecl$ on a separate page.}

\hardsubproblem{You are given $\mathbb{D}$ with $n$ training points $<x_i, y_i>$ but now you are also given a set of weights $\bracks{w_1~w_2~ \ldots ~w_n}$ which indicate how costly the error is for each of the $i$ points. Rederive the least squares estimates $b_0$ and $b_1$ under this situation. Note that these estimates are called the \emph{weighted least squares regression} estimates. This variant $\mathcal{A}$ on OLS has a number of practical uses, especially in Economics. No need to simplify your answers like I did in class (i.e. you can leave in ugly sums.}\spc{16.5}


\extracreditsubproblem{Interpret the ugly sums in the $b_0$ and $b_1$ you derived above and compare them to the $b_0$ and $b_1$ estimates in OLS. Does it make sense each term should be altered in this matter given your goal in the weighted least squares?}


\end{enumerate}

\end{document}





