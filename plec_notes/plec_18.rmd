---
title: "Lecture 18 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 18, 2018"
output:
  html_document:

  pdf_document: default
  word_document: default
---

# Forward Stepwise Linear Model Construction

Let's look at the diamonds data

```{r}

pacman::p_load(ggplot2)
data(diamonds)
diamonds$cut = factor(as.character(diamonds$cut))
diamonds$color = factor(as.character(diamonds$color))
diamonds$clarity = factor(as.character(diamonds$clarity))
```

What we're doing will be highly computational, so let's take a random sample of the dimaonds in $\mathbb{D}$:

```{r}
Nsamp = 1300
train_indices = sample(1 : nrow(diamonds), Nsamp)
diamonds_train = diamonds[train_indices, ]
```



Let's built a model with all second-order interactions

```{r}
mod = lm(price ~ . * . * ., diamonds_train)
```

How many variables is this? And what does it look like?

```{r}
length(coef(mod))
coef(mod)[1000 : 1100]
```

Remember we overfit just using first order interactions? We'll certainly overfit using first-order interactions AND second order interactions

```{r}
summary(mod)$r.squared
sd(summary(mod)$residuals)
```

Is that believable? Well... let's try it on the another 10,000 we didn't see...

```{r}
test_indices = sample(setdiff(1 : nrow(diamonds), train_indices), Nsamp)
diamonds_test = diamonds[test_indices, ]
y_hat_test = predict(mod, diamonds_test)
y_test = diamonds_test$price
e_test = y_test - y_hat_test
1 - sum((e_test)^2) / sum((y_test - mean(y_test))^2)
sd(e_test)
```

VERY negative oos $R^2$ --- why? What should that say about the relationship between $s_e$ and $s_y$?

```{r}
sd(y_test)
sd(e_test) / sd(y_test)
```

This is not only "overfitting"; it is an absolute trainwreck if you can do better using the null model (average of y) instead of your model.

So let us employ stepwise to get a good model. We need predictors to start with. How about `. * . * .` --- there's nothing intrinsically wrong with this. Let's create the model matrix:

```{r}
Xmm_train = model.matrix(price ~ . * . * ., diamonds_train)
y_train = diamonds_train$price
p_plus_one = ncol(Xmm_train)

Xmm_test = model.matrix(price ~ . * . * ., diamonds_test)
```

Now let's go through one by one and add the best one based on $s_e$ gain i.e. the best new dimension to add to project the most of the vector $y$ as possible onto the column space.

```{r eval=FALSE, include=FALSE}
predictor_by_iteration = c() #keep a growing list of predictors by iteration
in_sample_ses_by_iteration = c() #keep a growing list of se's by iteration
oos_ses_by_iteration = c() #keep a growing list of se's by iteration
i = 1

while (TRUE){
  #get all predictors left to try
  all_ses = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (!(j_try %in% predictor_by_iteration)){
      Xmm_sub = Xmm_train[, c(predictor_by_iteration, j_try), drop = FALSE]
      all_ses[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #lm.fit so much faster than lm! 
    }
  }
  j_star = which.min(all_ses)
  predictor_by_iteration = c(predictor_by_iteration, j_star)
  in_sample_ses_by_iteration = c(in_sample_ses_by_iteration, all_ses[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, predictor_by_iteration, drop = FALSE]
  mod = lm.fit(Xmm_sub, y_train)
  y_hat_test = Xmm_test[, predictor_by_iteration, drop = FALSE] %*% mod$coefficients
  oos_se = sd(y_test - y_hat_test)
  oos_ses_by_iteration = c(oos_ses_by_iteration, oos_se)
  
  cat("i = ", i, "in sample: se = ", all_ses[j_star], "oos_se", oos_se, "\n   predictor added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  predictor_by_iteration
  
  if (i > Nsamp || i > p_plus_one){
    break #why??
  }
  
}
```

Now let's look at the patterns

```{r}
simulation_results = data.frame(
  iteration = 1 : length(in_sample_ses_by_iteration),
  in_sample_ses_by_iteration = in_sample_ses_by_iteration,
  oos_ses_by_iteration = oos_ses_by_iteration
)

pacman::p_load(latex2exp)
ggplot(simulation_results) + 
  geom_line(aes(x = iteration, y = in_sample_ses_by_iteration), col = "red") +
  geom_line(aes(x = iteration, y = oos_ses_by_iteration), col = "blue") + 
  ylab(TeX("$s_e$"))
```


We can kind of see what the optimal model is above. If we want an exact procedure, we'd probably fit a separate smoothing regression to the oos results and analytically find the arg-minimum, $j^*$. That number will then be fed into the model matrix to create the right feature set and the final model will be produced with all the data.

Note that we will need a third split to honestly assess future performance as the test set above became our selection set because I was being sloppy.
