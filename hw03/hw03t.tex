\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
%\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line

\newcommand{\ev}{\vec{e}}
\newcommand{\wv}{\vec{w}}

\title{MATH 390.4 / 650.2 Spring 2018 Homework \#3t}

\author{Rebecca Strauss} %STUDENTS: write your name here


\begin{document}
\maketitle

\thispagestyle{empty}
\vspace{1cm}


\problem{These are questions about Silver's book, chapter 2.}


\begin{enumerate}

\intermediatesubproblem{If one's goal is to fit a model for a phenomenon $y$, what is the difference between the approaches of the hedgehog and the fox? Answer using notation from class (i.e. $t ,f, g, h^*, \delta, \epsilon, e, t, z_1, \ldots, z_t, \mathbb{D}, \mathcal{H}, \mathcal{A}, \mathcal{X}, \mathcal{Y}, X, y, n, p, x_{\cdot 1}, \ldots, x_{\cdot p}, x_{1 \cdot}, \ldots, x_{n \cdot}$, etc.). Connecting this to the modeling framework should really make you think about what Tetlock's observation means for political and historical phenomena.}\spc{4}

Hedgehog would (mistakenly) be searching for the 'absolute truth', $y=t(z_1,..,z_n)$, since they are "order-seekeing". They probably don't account for $\delta$ ignorance error, since they are "stubborn"-mistakes are blamed on bad luck. Hedgehogs overfit their models, by incorparting new $x^*$ - "Stalwart"\\
Foxes are "empirical"-uses better observations for their historical records$\mathcal{D}$. "Adatable"-try different $\mathcal{H}$ and $\mathcal{A}$. Foxes know they are looking for $h^*$, (aware of all types of eror)"tolerant of complexity" %{table on page 56}

\easysubproblem{Why did Harry Truman like hedgehogs? Are there a lot of people that think this way?}\spc{4}

Harry Truman liked hedgehogs more because they're able to given a defintive answer with confidence, while foxes return ranges and porbabilites. A lot of people would prefer to listen to hedgehogs because of their confidence and "size" of prediction. 	
\hardsubproblem{Why is it that the more education one acquires, the less accurate one's predictions become?}\spc{4}


\easysubproblem{Why are probabilistic classifiers (i.e. algorithms that output functions that return probabilities) better than vanilla classifiers (i.e. algorithms that only return the class label)? We will move in this direction in class soon.}\spc{4}

\end{enumerate}

\problem{These are questions about Finlay's book, chapter 2-4. We will hold off on chapter 1 until we cover probability estimation after midterm 2.}


\begin{enumerate}

\easysubproblem{What term did we use in class for \qu{behavioral (outome) data}?}\spc{0}

Response data

\easysubproblem{Write about some reasons why data scientists implement models that are subpar in predictive performance (p27).}\spc{3} 

Data scientist might implent a model that is subpar in predictive performance because it is simple and easy to explain. Even though the data scientist builds the model, someone without a statistical background might be using it for their job. In this case, it won't matter how well the model can predict if the user can't comprehend what the predictions mean.

\easysubproblem{In the first wine example, what is the outcome metric and what kind of supervised learning was employed?}\spc{0}

In finlays first example the outcome metric was the response rate of people buying wine and they employed  classification. 
\easysubproblem{In the second wine example, what is the outcome metric and kind of supervised learning was employed?}\spc{0}

In the second wine problem, the outcome metric was gross profit on a case of wine, given that someone responded to the original campaign.REGREEION

\easysubproblem{In the third chapter, why is it that some organizations cannot use predictive modeling to improve their business?}\spc{3}

Some organizations cannot implent predictive models to improve bussiness because they are unwilling to fully commit to the "model lifestyle"-implementing predictive anayltics for the first time requires a lot of money, IT power, and change. (the problem isnt building the model it's using the model.) Additionally, when a new model is implemented, the people using the model don't understand what the outcome metric means and will subsititute their own opinion in. Also, if I'm a generic worker at some company, and I'm given a model to "help" with my job, if I see the model working succesfully,  I may misreport the results so I don't loss my job.
\easysubproblem{In the bankruptcy case, what is the problem with merely using $g$ to obtain a $\hat{y}$ without any other information from the model?}\spc{3}

In the bankruptcy case, the bank forgot to take into account the real world limitations of acting on their prediction $g$. On average, for every 50 reviews the bank had to conduct, only 1 would voluntary foreclose. Even though the model was returning accurate predictions, each review process took about 2.5 hours. Hence, implementing the model on $g$ alone is not cost effective.
\easysubproblem{Chapter 3 talks about using the model with human judgment. Under what circumstances is this beneficial? Answer using notation from class (i.e. $t ,f, g, h^*, \delta, \epsilon, e, t$, $z_1, \ldots, z_t, \mathbb{D}, \mathcal{H}, \mathcal{A}, \mathcal{X}, \mathcal{Y}, X, y, n, p, x_{\cdot 1}, \ldots, x_{\cdot p}, x_{1 \cdot}, \ldots, x_{n \cdot}$, etc.).}\spc{3}

HUman judgement is beneficial to model-based predictions when the human can bring some outside information that is relevant to the predictions but that is not inferrable from the data. In the wine example, our $\mathcal{D}$ has a large $p$ consisting of contact details, age, income, martial status... The classification model breaks up age into 3 classes: $\leq34, 35-54, \geq55$. Without applying any human judgement to this model, people can end up on our wine campaign list who are not old enough to legally buy wine. Wastefull\\

\hardsubproblem{In Chapter 4 Finaly makes an interesting observation based on his experience in data science. He says most predictive models have $p \leq 30$. Why do you think this is? Discuss.}\spc{5}


\easysubproblem{He says there is \qu{almost always other data that could be acquired ... [which] doesn't always come for free}. The \qu{data} he is talking about here specifically means \qu{more predictors} i.e. increasing $p$. In what cases would someone be willing to pay for this data?}\spc{3}


\easysubproblem{Table 4 lists \qu{data types} about what type of observations?}\spc{1}
Qualitative observations
\easysubproblem{What type of data does he find in his experience to be the most important to predictive modeling? Why do you think this is so?}\spc{3}

Finlay finds primary behaviors to be the most important data type in predictive modeling. Primary behaviors refer to past behaviors that are similiar to the behavior you are trying to model. This is important data because if you've done something once, you are likely to do it again. If you are someone who  borrows money from your friends and always "forgets" to pay them back, the next time you borrow money, you probably wont pay it back.
\easysubproblem{If $x_{\cdot 17}$ was age and $x_{\cdot 18}$ is age of spouse, what is the most likely reason why adding $x_{\cdot 18}$ to $\mathbb{D}$ not be friutful for predictive ability?}\spc{3}

You wouldn't gain any useful information because there's about a 90\% of your spouse being +-5 years away from your age(spouses age is a function of your age, usually)
\hardsubproblem{What is the lifespan of a predictive model? Why does it not last forever? Answer using notation from class (i.e. $t ,f, g, h^*, \delta, \epsilon, e, t$, $z_1, \ldots, z_t, \mathbb{D}, \mathcal{H}, \mathcal{A}, \mathcal{X}, \mathcal{Y}, X, y, n, p$, $x_{\cdot 1}, \ldots, x_{\cdot p}, x_{1 \cdot}, \ldots, x_{n \cdot}$, etc.).}\spc{3}

The lifespan of a predictive model is finite. It's lifespan has come to an end when "the relationships that were found between the predictor data and the outcome data when the model was orginially constructed no longer apply"


\hardsubproblem{What does \qu{large enough to representative of the full population} (p80) mean? Answer using notation from class (i.e. $t ,f, g, h^*, \delta, \epsilon, e, t$, $z_1, \ldots, z_t, \mathbb{D}, \mathcal{H}, \mathcal{A}, \mathcal{X}, \mathcal{Y}, X, y, n, p$, $x_{\cdot 1}, \ldots, x_{\cdot p}, x_{1 \cdot}, \ldots, x_{n \cdot}$, etc.).}\spc{3}

\easysubproblem{Is there a hype about \qu{big data} i.e. including millions of observations instead of a few thousand? Discuss Finlay's opinion.}\spc{3}

Finlays opinion regarding the hype about "big data"-huge benefits from using massive amounts of data, is a myth. Finlay says, in his own experince, once you go above a sample size of 10,000, the benefits you gain from adding more samples is marginal. Increasing from 10,000 to 100,000 can give you a 1-3\% increase in predictive ability. However, the extra storage capacity and processing power needed to ruN predictions on a sample this large, might not make adding these extra samples worth it.

\easysubproblem{What is Finlay's solution to \qu{overfitting} (p84)?}\spc{5}
Finlays solution to overfitting is to increase the sample size
\end{enumerate}


\problem{These are questions about association and correlation.}


\begin{enumerate}

\easysubproblem{Give an example of two variables that are both correlated and associated by drawing a plot.}\spc{4}

\begin{figure}[htp]
\centering
\includegraphics[width=3.8in]{prob3a.jpg}
\end{figure}



\easysubproblem{Give an example of two variables that are not correlated but are associated by drawing a plot.}\spc{10}
\begin{figure}[htp]

\includegraphics[width=3.8in]{prob3b.jpg}
\end{figure}

\easysubproblem{Give an example of two variables that are not correlated nor associated by drawing a plot.}\spc{4}
\begin{figure}[htp]

\includegraphics[width=3.8in]{prob3c.jpg}
\end{figure}

\easysubproblem{Can two variables be correlated but not associated? Explain.}\spc{4}
No. Correlation is a type of association.
\end{enumerate}
\newpage
\problem{These are questions about multivariate linear model fitting using the least squares algorithm.}
\begin{enumerate}

\hardsubproblem{Derive $\partialop{\c}{\c^\top A \c}$ where $\c \in \reals^n$ and $A \in \reals^{n \times n}$ but \textit{not} symmetric. Get as far as you can.}\spc{8}
\begin{equation*}
A\vec{c}= \begin{bmatrix}
a_{11}c_1+a_{12}c_2\cdots+a_{1n}c_n\\
a_{21}c_1+a_{22}c_2\cdots+a_{2n}c_n\\
\vdots\\
a_{n1}c_1+a_{n2}c_2\cdots+a_{nn}c_n
\end{bmatrix}\in\mathbb{R}^{n\times1}
\end{equation*}
Then \begin{equation*}
\vec{c}^T(A\vec{c})=c_1(a_{11}c_1+a_{12}c_2\cdots+a_{1n}c_n)+c_2(a_{21}c_1+a_{22}c_2\cdots+a_{2n}c_n)\cdots+c_n(a_{n1}c_1+a_{n2}c_2\cdots+a_{nn}c_n)
\end{equation*}
Take derivative with respect to $c_1$\begin{eqnarray*}
\frac{\partial}{\partial c_1}=(2a_{11}c_1+a_{21}c_2\cdots+a_{1n}c_n)+a_{21}c_2+a_{31}c_3\cdots+a_{n1}c_n\\=2a_{11}c_1+c_2(a_{12}a_{21})+c_3(a_{13}a_{31})\cdots+c_n(a_{1n}a_{n1})
\end{eqnarray*}
Take derivative with respect to $c_2$\begin{eqnarray*}
\frac{\partial}{\partial c_2}=(2a_{22}c_2+a_{21}c_1\cdots+a_{2n}c_n)+a_{12}c_1+a_{32}c_3\cdots+a_{n2}c_n\end{eqnarray*}

\easysubproblem{Given matrix $X \in \reals^{n \times (p+1)}$, full rank and first column consisting of the $\onevec_n$ vector, rederive the least squares solution $\b$ (the vector of coefficients in the linear model shipped in the prediction function $g$). No need to rederive the facts about vector derivatives.}\spc{10}%check answer

\begin{equation*}
SSE=\sum\limits_{i=1}^n(\vec{y_i}-\vec{\hat{y_i}})^2
 \end{equation*}
 which we can rewrite as \begin{equation*}
 (\vec{y}-\vec{\hat{y}})^T(\vec{y}-\vec{\hat{y}})=(\vec{y}^T-\vec{\hat{y}}^T)(\vec{y}-\vec{\hat{y}})
 \end{equation*}
 Foil
 \begin{equation*}
 \vec{y}^T\vec{y}-\vec{y}^T\vec{\hat{y}}-\vec{\hat{y}}^T\vec{y}+\vec{\hat{y}}^T\vec{\hat{y}}
\end{equation*}
Replace $\vec{y}^T\vec{\hat{y}}=\vec{y}\vec{\hat{y}}^T$
\begin{equation*}
\vec{y}^T\vec{y}-2\vec{y}\vec{\hat{y}}^T+\vec{\hat{y}}^T\vec{\hat{y}}
\end{equation*}
Replace $\vec{\hat{y}}=X\vec{w}$
\begin{equation*}
SSE=\vec{y}^T\vec{y}-2\vec{y}(X\vec{w})^T+(X\vec{w})^T(X\vec{w})
\end{equation*}
Take partials
\begin{equation*}
\frac{\partial}{\partial\vec{w}}[SSE]=\frac{\partial}{\partial\vec{w}}[\vec{y}^T\vec{y}]-2\frac{\partial}{\partial\vec{w}}[\vec{y}X^T\vec{w}^T]+\frac{\partial}{\partial\vec{w}}[X^TX\vec{w}^T\vec{w}]
\end{equation*}
\begin{equation*}
\frac{\partial}{\partial\vec{w}}[SSE]= \vec{0}_{p+1}-2X^T\vec{y}+2X^TX\vec{w}
\end{equation*}
Set $\frac{\partial}{\partial\vec{w}}[SSE]=0$, factor out the 2
\begin{align*}
-X^T\vec{y}+X^TX\vec{w}=0 \\
X^TX\vec{w}=X^T\vec{y}
\end{align*}
Since $X$ has full rank $p+1$, we know that $X^TX$ is invertible, and Let $\vec{w}=\vec{b}$
\begin{equation*}
\vec{b}=(X^TX)^{-1}X^T\vec{y}
\end{equation*}


\intermediatesubproblem{Consider the case where $p = 1$. Show that the solution for $\b$ you just derived is the same solution that we proved for simple regression in Lecture 8. That is, the first element of $\b$ is the same as $b_0 = \ybar - r \frac{s_y}{s_x}\xbar$ and the second element of $\b$ is $b_1 = r \frac{s_y}{s_x}$.} \spc{10}

\begin{eqnarray}
b_1=r\frac{s_y}{s_x}=\frac{(n-1)S_{xy}}{(n-1)S_{xx}}=\frac{\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum\limits_{i=1}^nx_i^2-n\bar{x}^2}\\
b_0=\bar{y}-r\frac{s_y}{s_x}\bar{x}
\end{eqnarray}
 From the previous question, we know that $\vec{b}=(X^TX)^{-1}X^T\vec{y}$ and since $p=1$, $X\in\mathbb{R}^{n\times2}$ 
\begin{equation*}
X=\begin{bmatrix}1 && x_1\\1 && x_2\\\cdots\\1 && x_n\end{bmatrix}\\ 
X^T= \begin{bmatrix}1 && 1 && \cdots && 1\\x_1 && x_2 && \cdots && x_n\end{bmatrix} \\
\vec{y}=\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}
\end{equation*}
using  (note:$\sum\limits_{i=1}^ny_i=n\bar{y}$) 
\begin{equation*}X^T\vec{y}=
\begin{bmatrix}1&&1&&\cdots&&1\\x_1&&x_2&&\cdots&&x_n\end{bmatrix}
\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}=
\begin{bmatrix}\sum\limits_{i=1}^ny_i\\\sum\limits_{i=1}^nx_iy_i \end{bmatrix}=
\begin{bmatrix}n\bar{y}\\\sum\limits_{i=1}^nx_iy_i \end{bmatrix}
\end{equation*}

Now mutiply(note:$\sum\limits_{i=1}^nx_i=n\bar{x}$) \begin{equation*}X^TX= \begin{bmatrix}1 && 1 && \cdots && 1\\x_1 && x_2 && \cdots && x_n\end{bmatrix}
\begin{bmatrix}1 && x_1\\1 && x_2\\\cdots\\1 && x_n\end{bmatrix} = 
\begin{bmatrix}n && \sum\limits_{i=1}^nx_i\\\sum\limits_{i=1}^nx_i && \sum\limits_{i=1}^nx_i^2\end{bmatrix}=
\begin{bmatrix}n &&n\bar{x}\\n\bar{x}&&\sum\limits_{i=1}^nx_i^2\end{bmatrix}\end{equation*}
Since X has full rank we can take the inverse of it(note: $S_{xx}=\sum\limits_{i=1}^nx_i^2-n\bar{x}^2$)\begin{equation*}
\det{X^TX}=n\sum\limits_{i=1}^nx_i^2-(n\bar{x})^2=n(\sum\limits_{i=1}^nx_i^2-n\bar{x}^2)=nS_{xx}\end{equation*}
The inverse is then \begin{equation*}
(X^TX)^{-1}=\frac{1}{nS_{xx}}\begin{bmatrix}
\sum\limits_{i=1}^nx_i^2&&-n\bar{x}\\
-n\bar{x}&&n
\end{bmatrix}=\frac{1}{S_{xx}}\begin{bmatrix}
\frac{1}{n}\sum\limits_{i=1}^nx_i^2&&-\bar{x}\\
-\bar{x}&&1\end{bmatrix}\end{equation*}
Now we have  \begin{equation*}
\vec{b}=(X^TX)^{-1}X^T\vec{y}=\frac{1}{S_{xx}}\begin{bmatrix}
\frac{1}{n}\sum\limits_{i=1}^nx_i^2&&-\bar{x}\\
-\bar{x}&&1
\end{bmatrix}\begin{bmatrix}n\bar{y}\\\sum\limits_{i=1}^nx_iy_i \end{bmatrix}=
\frac{1}{S_{xx}}\begin{bmatrix}
\frac{1}{n}(\sum\limits_{i=1}^nx_i^2)(n\bar{y})-\bar{x}\sum\limits_{i=1}^nx_iy_i\\
-\bar{x}n\bar{y}+\sum\limits_{i=1}^nx_iy_i
\end{bmatrix}
\end{equation*}
\begin{equation*}
\vec{b}=\frac{1}{\sum\limits_{i=1}^nx_i^2-n\bar{x}^2}\begin{bmatrix}
\bar{y}\sum\limits_{i=1}^nx_i^2-\bar{x}\sum\limits_{i=1}^nx_iy_i\\
\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}
\end{bmatrix}
\end{equation*}
\begin{equation}
b_1=\frac{\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum\limits_{i=1}^nx_i^2-n\bar{x}^2}=r\frac{s_y}{s_x}
\end{equation}
EquatioN 3 matches equation 1 from above\\
Now sub $b_1$ into the equation for $b_0$
\begin{equation}
b_0=\frac{\bar{y}\sum\limits_{i=1}^nx_i^2-\bar{x}\sum\limits_{i=1}^nx_iy_i}{\sum\limits_{i=1}^nx_i^2-n\bar{x}^2}=\bar{y}-\bar{x}r\frac{s_y}{s_x}
\end{equation}


\easysubproblem{If $X$ is rank deficient, how can you solve for $\b$? Explain in English.} \spc{2}

Since $X$ is rank deficient, $b$ is not in the column space of $X$. Instead of solving exactly for $b$, we could find the element in column space of $X$ that is "closest" to $b$, which is its' projection. So we are looking for the hat matrix that gives us $H\vec{y}=Proj_b(X)$

\hardsubproblem{Prove $\rank{X} =\rank{X^\top X}$.}\spc{6}

\hardsubproblem{Given matrix $X \in \reals^{n \times (p+1)}$, full rank and first column consisting of the $\onevec_n$ vector, now consider cost multiples (\qu{weights}) $c_1, c_2, \ldots, c_n$ for each mistake $e_i$. As an example, previously the mistake for the 17th observation was $e_{17} := y_{17} - \hat{y}_{17}$ but now it would be $e_{17} := c_{17} (y_{17} - \hat{y}_{17})$.  Derive the weighted least squares solution $\b$. No need to rederive the facts about vector derivatives. Hints: (1) show that SSE is a quadratic form with the matrix $C$ in the middle (2) Split this matrix up into two pieces i.e. $C = C^{\half} C^{\half}$, distribute and then foil (3) note that a scalar value equals its own transpose and (4) use the vector derivative formulas.}\spc{20}


\hardsubproblem{If $p=1$, prove $r^2 = R^2$ i.e. the linear correlation is the same as proportion of sample variance explained in a least squares linear model.}\spc{6}


\intermediatesubproblem{Prove that the point $<1,\xbar_1, \xbar_2, \ldots, \xbar_p, \bar{y}>$ is a point on the least squares linear solution.}\spc{13}

\end{enumerate}

\problem{These are questions related to the concept of orthogonal projection, QR decomposition and its relationship with least squares linear modeling.}

\begin{enumerate}

\easysubproblem{Consider least squares linear regression using a design matrix $X$ with rank $p + 1$. What are the degrees of freedom in the resulting model? What does this mean?}\spc{3}


\intermediatesubproblem{If you are orthogonally projecting the vector $\y$ onto the column space of $X$ which is of rank $p + 1$, derive the formula for $\proj{\colsp{X}}{\y}$. Is this the same as the least squares solution?}\spc{6}

Let $\colsp{X}=[X_1, X_2\cdots, X_{p+1}]\in\mathbb{R}^{n\times{p+1}}$\\
Notes \begin{itemize}
\item (1) $\y=\proj{\colsp{X}}{\y}+\ev\\ \Rightarrow\ev=\y-\proj{\colsp{X}}{\y}$
\item (2)$\ev\cdot\proj{\colsp{X}}{\y}=\vec{0}$ and $\ev\cdot\colsp{X}=\vec{0}$
\end{itemize}
Since $\y$ is being orthogonally projected onto the column space of $X$, we know that $\proj{\colsp{X}}{\y}\in\colsp{X}$. In other words, $\exists$ a scalar $\wv$ s.t \begin{equation}
\proj{\colsp{X}}{\y}=w_1X_1+w_2X_2\cdots w_{p+1}X_{p+1}=X\wv
\end{equation}
By note 1 $\ev=\y-X\wv$ and by note 2 $X^T(\y-X\wv)=\vec{0}$
\begin{eqnarray*}
X^T(\y-X\wv)=\vec{0}\\
X^T\y-X^TX\wv=\vec{0}\\
X^T\y=X^TX\wv
\end{eqnarray*}
Since $X$ has full rank $p+1$, $X^TX$ is also of full rank which means we can take the inverse firecetly\begin{equation}
(X^TX)^{-1}X^T\y=\wv
\end{equation}
Now plug $\wv$ into equation 1 and we get \begin{equation}
\proj{\colsp{X}}{\y}= X(X^TX)^{-1}X^T\y
\end{equation}
Which is the same as the least squares solution.


\hardsubproblem{We saw that the perceptron is an \textit{iterative algorithm}. This means that it goes through multiple iterations in order to converge to a closer and closer $\w$. Why not do the same with linear least squares regression? Consider the following. Regress $\y$ using $\X$ to get $\yhat$. This generates residuals $\e$ (the leftover piece of $\y$ that wasn't explained by the regression's fit, $\yhat$). Now try again! Regress $\e$ using $\X$ and then get new residuals $\e_{new}$. Would $\e_{new}$ be closer to $\zerovec_n$ than the first $\e$? That is, wouldn't this yield a better model on iteration \#2? Yes/no and explain.}\spc{10}

Each residual, $e_i$, is calculated by $e_i=y_i-\hat{y}_i$ where $\hat{y}_i=X_iw_i$. Hence $\vec{e}$ is dependent on X. Running a regression on dependent variables will return a smaller error rate than if you ran the regression on independent variables. So, yes, this process will yield a better model on iteration 2.

\intermediatesubproblem{Prove that $Q^\top = Q^{-1}$ where $Q$ is an orthonormal matrix such that $\colsp{Q} = \colsp{X}$ and $Q$ and $X$ are both matrices $\in \reals^{n \times (p+1)}$. Hint: this is purely a linear algebra exercise.}\spc{10}

Calculate $Q^TQ=\begin{bmatrix}
\leftarrow q_1 \rightarrow\\
\leftarrow q_2 \rightarrow\\
\vdots\\
\leftarrow q_{p+1} \rightarrow\\
\end{bmatrix}\begin{bmatrix}
\uparrow & \uparrow & \cdots & \uparrow\\
q_1 & q_2 & \cdots & q_{p+1}\\
\downarrow & \downarrow & \cdots & \downarrow
\end{bmatrix}=\begin{bmatrix}
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\ddots\\
0 & 0 & \cdots & 1
\end{bmatrix}$
So $Q^TQ=I_{p+1}$. Linear algebra rules tell us that if we have a vector times its' own transpose that equals the identity matrix, the transpose of that matrix must be equal to the inverse. In basic math, if we have $xy=1$, without loss of generality, $x=y^{-1}$. Here we just have the vector version of this relationship. Hence $Q^T=Q^{-1}$ 

\intermediatesubproblem{Prove that the least squares projection $H = \XXtXinvXt$ is the same as $QQ^\top$.}\spc{10}

\intermediatesubproblem{Prove that an orthogonal projection onto the $\colsp{Q}$ is the same as the sum of the projections onto each column of $Q$.}\spc{10}


\hardsubproblem{Trouble in paradise. Prove that the SSE of a multivariate linear least squares model always decreases (equivalently, $R^2$ always increases) upon the addition of a new independent predictor. Keep in mind this holds true even if this new predictor has no information about the true causal inputs to the phenomenon $y$.}\spc{12}

\intermediatesubproblem{Why is this a bad thing? Explain in English.}\spc{3}



\extracreditsubproblem{Prove that $\rank{H} =\tr{H}$.}\spc{-0.5}

\end{enumerate}


\end{document}
